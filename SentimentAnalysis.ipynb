{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiZoK4sf5hu_"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoKgfLd5hBW"
      },
      "source": [
        "import torch,torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets \n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hej7n-yRDL7w"
      },
      "source": [
        "#Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRCnSVlF5QA_"
      },
      "source": [
        "First step is to generate biagram features as described in the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG6-2QVYhaGo"
      },
      "source": [
        "def bigramF(sentence):\n",
        "  '''\n",
        "  l=sentence\n",
        "  unigrams=[]\n",
        "  bigrams= []\n",
        "\n",
        "  for i in range(len(l)):\n",
        "    \n",
        "    unigrams.append( l[i])\n",
        "    if i <len(l)-1:\n",
        "       \n",
        "      bigrams = bigrams +[l[i]+' '+l[i+1]]\n",
        "  return unigrams+bigrams\n",
        "  '''\n",
        "   \n",
        "  for ngram in set(zip(*[sentence[i:]for i in range(2)])):\n",
        "    sentence.append(' '.join (ngram))\n",
        "  return sentence"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqyGvOEs5or4"
      },
      "source": [
        "Example of how this looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_jjzAstiyBM",
        "outputId": "9e14dee7-9db9-4fcb-d118-8feedd962194"
      },
      "source": [
        "sentence='this film is great'\n",
        "sent=sentence.split()\n",
        "bigramF(sen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'film', 'is', 'great', 'this film', 'film is', 'is great']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKJebFYS5u71"
      },
      "source": [
        "Setting the random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIgolxUSGt__"
      },
      "source": [
        "def setSeed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "seed=124\n",
        "setSeed(seed)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn2eDMc_52YV"
      },
      "source": [
        "Then we initialize the torch.text data Field object to build/tokenize & preprocess our texts and labels .Then we use IMDB dataset that is already available on torch.text and build the vocabulary of this data . The output is a dictionary with words indexed and its glove 100d pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy-OSuI9C_Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02344540-2c87-42f2-c449-59c98be031bf"
      },
      "source": [
        "TEXT=data.Field(tokenize='spacy',tokenizer_language='en_core_web_sm',preprocessing=bigramF)\n",
        "LABEL=data.LabelField(dtype=torch.float)\n",
        "\n",
        "# make splits for data\n",
        "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
        "# build the vocabulary\n",
        "\n",
        "TEXT.build_vocab(train,unk_init=torch.normal,max_size=25000,vectors='glove.6B.100d')\n",
        "LABEL.build_vocab(train)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 28.7MB/s]\n",
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399617/400000 [00:17<00:00, 21235.96it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irq2bc7m5_vR"
      },
      "source": [
        "We split traindata , and then load the iterator to loop through data in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7r5_Ies9_Ez"
      },
      "source": [
        "traindata ,  valid = train.split(random_state=random.seed(seed))\n",
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "trainLoader,validLoader,testLoader=data.BucketIterator.splits(datasets=(traindata,valid,test),batch_size=64,device=device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbKHXybkDPKs"
      },
      "source": [
        "#Build the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9RkfCGk7eJ3"
      },
      "source": [
        "The model is implemented as in the paper , Embedding layer at first and then the output is averaged across the input dimension(which is vocabulary size ) using average pooling filter .Then a linear layer follows to return the final output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efyrZ5w3a_tz"
      },
      "source": [
        "class FastText(nn.Module):\n",
        "  def __init__(self,inputdim,embeddim,hiddim,outputdim,pad_idx):\n",
        "      super().__init__()\n",
        "      self.embedding=nn.Embedding(inputdim,embeddim,padding_idx=pad_idx)\n",
        "      self.linear=nn.Linear(embeddim,outputdim)\n",
        "      \n",
        "  def forward(self,input):\n",
        "    ##print('input ',input.shape)\n",
        "    embed=self.embedding(input)\n",
        "   \n",
        "    embed=embed.permute(1,0,2)\n",
        "    avg=F.avg_pool2d(embed,(embed.shape[1],1)).squeeze(1)\n",
        "    #print('avg ',avg.shape)\n",
        "    output=self.linear(avg)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEtylV7UDRgI"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYPijnvU-qGM"
      },
      "source": [
        "Defining attributes of the model  and the train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqYTanGDo_Zy"
      },
      "source": [
        "inputdim=len(TEXT.vocab)\n",
        "embeddim=100\n",
        "hiddim=10\n",
        "outputdim=1\n",
        "pad_idx=TEXT.vocab.stoi[TEXT.pad_token]\n",
        "model=FastText(inputdim,embeddim,hiddim,outputdim,pad_idx)\n",
        "model.to(device)\n",
        "optimizor=torch.optim.Adam(model.parameters())\n",
        "criterion=nn.BCEWithLogitsLoss()\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms64lfrzvGoI"
      },
      "source": [
        "def train(loader,model,criterion,optimizor):\n",
        "  model.train()\n",
        "  epoch_loss=0\n",
        "  loss=0\n",
        "  accuracy=0\n",
        "  for batch in loader:\n",
        "    optimizor.zero_grad()\n",
        "\n",
        "    output=model(batch.text)\n",
        "    \n",
        "    loss=criterion(output.squeeze(1),batch.label)\n",
        "    loss.backward()\n",
        "    optimizor.step()\n",
        "    epoch_loss+=loss\n",
        "    accuracy+=acc(output.squeeze(1),batch.label)\n",
        "  print(len(loader))\n",
        "  return epoch_loss/len(loader),accuracy/len(loader)\n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUkbrkIFy2m2"
      },
      "source": [
        "def evaluate(loader,model,criterion):\n",
        "  model.eval()\n",
        "  epoch_loss,loss=0, 0\n",
        "  accuracy=0\n",
        "  for batch in loader:\n",
        "    with torch.no_grad():\n",
        "   \n",
        "      output=model(batch.text)\n",
        "      loss=criterion(output.squeeze(1),batch.label)\n",
        "      epoch_loss+=loss\n",
        "      accuracy+=acc(output.squeeze(1),batch.label)\n",
        "    \n",
        "  return epoch_loss/len(loader) ,accuracy/len(loader)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVTenYKY_4kI"
      },
      "source": [
        "#Binary Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmeuFLcm_-7l"
      },
      "source": [
        "A function to calculate sum of correct predicted classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfGoBs2-_57i"
      },
      "source": [
        "def acc(predicted, groundtruth):\n",
        "  \n",
        "  predicted=torch.sigmoid(predicted)\n",
        "  predicted [predicted>0.5]=1\n",
        "  predicted[predicted<=0.5]=0\n",
        "\n",
        "  return torch.sum(torch.eq(predicted.detach(),groundtruth))/len(predicted)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3LSa6cTof1a"
      },
      "source": [
        "#Epoch time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPMsXSh8oiD4"
      },
      "source": [
        "import time \n",
        "def epochT(start, end):\n",
        "  min=int(end-start)/60\n",
        "  sec=int((end-start) - min*60)\n",
        "  return min, sec"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KWBR1Lq-2BX"
      },
      "source": [
        "Running the model for a number of epochs and output the best validation accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TArWA41559Gt",
        "outputId": "700ffe9f-cfd5-44bc-b5ae-af1084a70a2c"
      },
      "source": [
        "best_loss=float('inf')\n",
        "for epoch in range(5):\n",
        "  start=time.time()\n",
        "  trainloss,train_acc=train(trainLoader,model,criterion,optimizor)\n",
        "  validloss,val_acc=evaluate(validLoader,model,criterion)\n",
        "  end=time.time()\n",
        "  min,sec=epochT(start,end)\n",
        "  print(f'train loss is :{trainloss} and train_accuracy is {train_acc} trained on {min} minutes and {sec} seconds')\n",
        "  print(f'validation loss is {validloss} and validation_accuracy is {val_acc}evaluated on {min} minutes and {sec} seconds ')\n",
        "  if validloss<best_loss:\n",
        "    best_loss=validloss\n",
        "    torch.save(model.state_dict(),'SentimentModel.pt')\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "274\n",
            "train loss is :0.6897445917129517 and train_accuracy is 0.573530375957489 trained on 1.1333333333333333 minutes and 0 seconds\n",
            "validation loss is 0.6590437293052673 and validation_accuracy is 0.630517303943634evaluated on 1.1333333333333333 minutes and 0 seconds \n",
            "274\n",
            "train loss is :0.6682575941085815 and train_accuracy is 0.7148722410202026 trained on 1.1166666666666667 minutes and 0 seconds\n",
            "validation loss is 0.5542584657669067 and validation_accuracy is 0.7680967450141907evaluated on 1.1166666666666667 minutes and 0 seconds \n",
            "274\n",
            "train loss is :0.6173906922340393 and train_accuracy is 0.7792622447013855 trained on 1.1166666666666667 minutes and 0 seconds\n",
            "validation loss is 0.45444488525390625 and validation_accuracy is 0.7948446273803711evaluated on 1.1166666666666667 minutes and 0 seconds \n",
            "274\n",
            "train loss is :0.5505263805389404 and train_accuracy is 0.8229112029075623 trained on 1.1666666666666667 minutes and 0 seconds\n",
            "validation loss is 0.4154271185398102 and validation_accuracy is 0.8173111081123352evaluated on 1.1666666666666667 minutes and 0 seconds \n",
            "274\n",
            "train loss is :0.48653724789619446 and train_accuracy is 0.8514403104782104 trained on 1.15 minutes and 0 seconds\n",
            "validation loss is 0.392353355884552 and validation_accuracy is 0.839954137802124evaluated on 1.15 minutes and 0 seconds \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IID4Kd_vn3UM"
      },
      "source": [
        "#Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "IiEmFLu1n5z5",
        "outputId": "2335c056-cf1f-47e4-cf85-b04579810367"
      },
      "source": [
        "model.load_state_dict(torch.load('SentimentModel.pt'))\n",
        "_,acc=evaluate(testLoader,model,criterion)\n",
        "print(f'Test accuracy is {acc} ')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-4650c0fb23be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SentimentModel.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test accuracy is {acc} '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-9b93870f60c7>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(loader, model, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mepoch_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0maccuracy\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cri3_d_DbFn"
      },
      "source": [
        "#User input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc3gr7XQ_TCw"
      },
      "source": [
        "classify a user input if positive or negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iNxgacI71gW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "45f25b1a-0119-4924-cf5e-8b3e9b6d0a57"
      },
      "source": [
        "lang=spacy.load('en_core_web_sm')\n",
        "def predict(sentence,model):\n",
        "  tokenized=[token.text for token in lang.tokenizer(sentence)]\n",
        "  preprocessed=bigramF(tokenized)\n",
        " \n",
        "  onehot=[TEXT.vocab.stoi[s] for s in preprocessed]\n",
        "  tensor=torch.LongTensor(onehot).to(device)\n",
        "  tensor=tensor.unsqueeze(1)\n",
        " \n",
        "  output=torch.sum(torch.sigmoid(model(tensor)) > 0.5)\n",
        "  print(output.item())\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8ec1363fd10c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpreprocessed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigramF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9SB3Utj_W5n",
        "outputId": "28d6cf3a-ca70-472e-cc2f-9c270eb2963d"
      },
      "source": [
        "\n",
        "input='this movie really good  , '\n",
        "model.load_state_dict(torch.load('SentimentModel.pt'))\n",
        "predict(input ,model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input  torch.Size([9, 1])\n",
            "avg  torch.Size([1, 100])\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}